# ğŸš€ e2e-elt-pipeline-airflow â€” Airflow ELT Pipeline (NASA API â†’ PostgreSQL)

This repository implements a production style ELT pipeline orchestrated with Apache Airflow. The pipeline extracts JSON data from an external REST API (NASA APOD â€“ Astronomy Picture of the Day), performs lightweight transformation, and loads curated records into a PostgreSQL database. The workflow is containerized using Docker for reproducibility across environments.

## ğŸŒ Overview

This project demonstrates how to build a production-style data engineering workflow with:

- Workflow orchestration using Apache Airflow DAGs

- Automated scheduling and monitoring via the Airflow UI

- REST API extraction using Airflow HTTP operators

- JSON transformation using Python TaskFlow tasks

- Database loading into PostgreSQL using Airflow Hooks/Operators

- Containerized services using Docker (Airflow + Postgres)

### ğŸ›°ï¸ Problem Context: API-Based Data Ingestion

Many modern data pipelines rely on ingesting data from external APIs (e.g., analytics, telemetry, finance, observability, public datasets). These sources often deliver information as JSON responses, which must be extracted, cleaned, validated, and stored for downstream analysis.

This project simulates a real-world ingestion pipeline where Airflow is used to:

- Fetch structured API data on a schedule

- Process/standardize JSON fields

- Persist records into a relational database for reporting and querying

### ğŸ“¡ Dataset / Data Source

This pipeline extracts data from:

NASA APOD API (Astronomy Picture of the Day)

Each daily API response contains structured metadata, including:

```bash
title
explanation
url
date
media_type
```

The pipeline stores these fields in PostgreSQL for historical querying and analysis.

### âœ¨ Key Features

#### ğŸ§© Airflow DAG Orchestration

- DAG-based pipeline execution with task dependencies

- Scheduled execution (daily pipeline runs)

<b>Airflow UI support for:</b>

- Run history

- Task status monitoring

- Logs and troubleshooting

#### ğŸŒ API Extraction (HTTP Operator)

- Uses Airflowâ€™s SimpleHttpOperator to fetch JSON payloads

- Handles API integration in a reusable Airflow pattern

#### ğŸ”„ Transformation (TaskFlow API)

- Transforms raw JSON into a cleaned structured object

- Extracts relevant fields and enforces formatting consistency

#### ğŸ—„ï¸ Load into PostgreSQL (Hook + Operator)

- Uses PostgresHook / PostgresOperator to interact with PostgreSQL

- Creates the target table automatically if it doesnâ€™t exist

- Loads transformed records into structured relational storage

### ğŸ³ Dockerized Services

- Airflow and PostgreSQL run as isolated services through Docker

- Environment is reproducible across machines

- Database persistence supported through Docker volumes

### ğŸ§± Pipeline Stages

The ELT pipeline consists of three core stages:

<b>1) Extract</b>

Calls NASAâ€™s APOD API via HTTP GET request

Returns metadata in JSON format

<b>2) Transform </b>

Parses JSON payload

Selects fields needed for storage

Normalizes values into a database-ready record

<b>3) Load </b>

Creates table if missing

Inserts transformed record into PostgreSQL

### ğŸ§  Tech Stack

Language: Python

Orchestration: Apache Airflow

Database: PostgreSQL

API Integration: REST APIs / JSON

Operators/Hooks: SimpleHttpOperator, TaskFlow API, PostgresHook

Containerization: Docker + Docker Compose

### ğŸ“ Repository Structure

```bash
e2e-elt-pipeline-airflow/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl.py               # Airflow DAG (Extract â†’ Transform â†’ Load)
â”‚
â”œâ”€â”€ docker-compose.yml       # Docker services (Postgres + pipeline environment)
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md
```

### ğŸ“‹ Setup & Usage

1. Start services (Docker)
   docker compose up -d

2. Open Airflow UI
   http://localhost:8080

3. Trigger the DAG

Enable the DAG in Airflow

Trigger a manual run OR allow schedule to execute automatically

### âš™ï¸ Workflow Architecture Summary

This project demonstrates a practical pipeline architecture commonly used in production environments:

API ingestion layer (HTTP extraction)

Transform layer (Python TaskFlow)

Storage layer (Postgres load)

Orchestration + monitoring layer (Airflow)
